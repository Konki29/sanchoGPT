# SanchoGPT üõ°Ô∏èüìñ

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![License](https://img.shields.io/badge/License-MIT-green)
![Status](https://img.shields.io/badge/Status-Experimental-yellow)

**SanchoGPT** es una implementaci√≥n minimalista y educativa de un modelo de lenguaje tipo Transformer (NanoGPT), entrenado desde cero para replicar el estilo y vocabulario del espa√±ol del Siglo de Oro, basado en el texto de *"El Ingenioso Hidalgo Don Quijote de la Mancha"*.

Este proyecto explora los fundamentos de la **Inteligencia Artificial Generativa** a bajo nivel: tokenizaci√≥n por caracteres, mecanismos de atenci√≥n y ajuste fino (Fine-Tuning) para chat.

![Demo](media/Animation.gif)
*(Visualizaci√≥n del proceso de generaci√≥n de texto)*

## üß† Arquitectura y Especificaciones

El modelo est√° construido sobre PyTorch siguiendo la arquitectura *Decoder-only Transformer* (similar a GPT-2).

| Par√°metro | Valor | Descripci√≥n |
| :--- | :--- | :--- |
| **Arquitectura** | NanoGPT | Implementaci√≥n basada en el trabajo de Andrej Karpathy. |
| **Tokenizaci√≥n** | Car√°cter (Char-level) | Vocabulario de ~90 tokens √∫nicos. |
| **Embeddings** | 192 dimensiones | Tama√±o del vector vectorial. |
| **Contexto** | 256 tokens | Ventana de atenci√≥n m√°xima. |
| **Capas / Cabezas** | 4 capas / 4 cabezas | Estructura del Transformer. |
| **Par√°metros** | ~0.15 M | Modelo extremadamente ligero. |
| **Hardware** | NVIDIA RTX 5070 Ti | Entrenamiento optimizado con CUDA. |

## üöÄ Caracter√≠sticas

- **Entrenamiento Base (Pre-training):** Aprende la gram√°tica y estilo de Cervantes a partir de texto plano.
- **Modo Chat (Fine-Tuning):** Capacidad experimental de responder preguntas siguiendo el formato `<|usuario|>` / `<|sancho|>`.
- **Inferencia Eficiente:** Generaci√≥n de texto en tiempo real en CPU o GPU.
- **Introspecci√≥n:** Scripts para exportar pesos a ONNX y visualizar embeddings en 3D.

## üõ†Ô∏è Instalaci√≥n

1. Clonar el repositorio:
   ```bash
   git clone [https://github.com/tu_usuario/sanchoGPT.git](https://github.com/tu_usuario/sanchoGPT.git)
   cd sanchoGPT
   ```

2. Crear entorno virtual e instalar dependencias:
   ```bash
   python -m venv venv
   source venv/bin/activate # o .\venv\Scripts\activate en Windows
   pip install torch matplotlib seaborn numpy
   ```

## üíª Flujo de Trabajo

El proyecto se divide en dos fases: Entrenamiento del Modelo Base y Ajuste para Chat.

### Fase 1: El Modelo Base (Escritor)

Entrena el modelo para que aprenda a escribir como Cervantes.

1. **Entrenar:**
   ```bash
   python model/sancho_model.py
   ```
   *Esto generar√° `model/ckpt.pt`.*

2. **Generar Texto (Inferencia):**
   ```bash
   python gen.py
   ```
   *Salida de ejemplo:*
   > "En un lugar de la mancha de cuyo nombre no quiero acordarme..."

### Fase 2: Sancho Chat (Conversacional) üß™

Una capa experimental para dotar al modelo de capacidad de interacci√≥n pregunta-respuesta.

1. **Preparar Dataset:**
   Convierte el dataset JSON a formato de texto con tokens especiales.
   ```bash
   python data/json2txt.py
   ```

2. **Fine-Tuning (SFT):**
   Refina el modelo base con el dataset de di√°logo.
   ```bash
   python entrenar_chat.py
   ```
   *Esto generar√° `sancho_chat.pt`.*

3. **Chatear:**
   Interfaz de consola interactiva.
   ```bash
   python chat_console.py
   ```

## üìä Visualizaci√≥n e Ingenier√≠a Inversa

Herramientas para entender qu√© ocurre dentro de la "caja negra".

### Mapa de Calor de Embeddings
Visualiza la similitud entre caracteres aprendida por el modelo.
```bash
python visualization/view.py
```
![Embeddings](media/model_internals.png)

### Exportaci√≥n a ONNX
Exporta el grafo computacional para inspecci√≥n en [Netron](https://netron.app/).
```bash
python visualization/onnx.py
```
![Arquitectura](media/sancho_architecture.onnx.png)

## ‚ö†Ô∏è Limitaciones y Aprendizajes

Este proyecto es una prueba de concepto acad√©mica con las siguientes limitaciones conocidas:
- **Tokenizaci√≥n por Caracteres:** Al no usar BPE (Byte Pair Encoding), el modelo tiene dificultades para mantener la coherencia sem√°ntica en frases largas o complejas.
- **Tama√±o del Dataset:** Entrenado con un corpus peque√±o (~150KB), lo que limita su "conocimiento del mundo" fuera del libro.
- **Alucinaciones:** En modo chat, puede inventar palabras o repetir bucles si la temperatura es alta.

## üìú Cr√©ditos

- Inspirado y guiado por la **"Gu√≠a Completa: C√≥mo Crear un Modelo GPT desde Cero"** de **Gabriel Merlo** ([Ver v√≠deo](https://youtu.be/QK4AHZTVf68)).
- Basado en la serie "Zero to Hero" de **Andrej Karpathy** y su repositorio [nanoGPT](https://github.com/karpathy/nanoGPT).
- Texto original: Project Gutenberg (Don Quijote).

---
*Desarrollado con fines educativos en Ingenier√≠a Rob√≥tica e IA.*